{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "import ollama\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9b7cb7c-b562-41c6-b450-6e320ad69170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# Initialize and constants\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your technical question:\n",
      " from openai import OpenAI from IPython.display import display, Markdown, update_display  def get_answer_from_llama():     ollama_via_openai = OpenAI(         base_url='http://localhost:11434/v1',  # Ollama's API endpoint         api_key='ollama'  # placeholder API key for local Ollama     )      stream = ollama_via_openai.chat.completions.create(         model=MODEL_LLAMA,         messages=[             {\"role\": \"system\", \"content\": system_prompt},             {\"role\": \"user\", \"content\": user_prompt}         ],         stream=True     )      accumulated = \"\"     display_handle = display(Markdown(\"\"), display_id=True)      for chunk in stream:         delta = chunk.choices[0].delta.content or \"\"         accumulated += delta         accumulated_clean = accumulated.replace(\"```\", \"\").replace(\"markdown\", \"\")         update_display(Markdown(accumulated_clean), display_id=display_handle.display_id)\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "question = input(\"Enter your technical question:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7639fbb7-8637-4f1a-b69a-2cdd8e19d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# question = \"\"\"\n",
    "# Please explain what this code does and why:\n",
    "# yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c832ed7a-31ab-4934-89eb-bcc48c8b0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the code \\\n",
    "and provides a short summary. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f574f89e-9727-48d0-9b06-3d7d20d44c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"You are given a code question {question} which you have to analyze and respond\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75b7b8c7-dc81-462b-b995-5a5b2d9745fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_open_ai():\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    accumulated = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content or ''\n",
    "        accumulated += delta\n",
    "        accumulated_clean = accumulated.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(accumulated_clean), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a1572a3-0270-4f44-ab8c-68f9ef829e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Code Summary\n",
       "\n",
       "The provided code is a Python function `get_answer_from_llama()` that communicates with a local instance of the Ollama API to retrieve and display chat completions based on user input.\n",
       "\n",
       "#### Key Components:\n",
       "\n",
       "1. **Libraries Imported**:\n",
       "   - `OpenAI`: To interact with the Ollama API.\n",
       "   - `IPython.display`: To update the notebook's display with  content in real-time.\n",
       "\n",
       "2. **API Configuration**:\n",
       "   - Connects to the Ollama API at `http://localhost:11434/v1` with a placeholder API key (`'ollama'`).\n",
       "\n",
       "3. **Message Structure**:\n",
       "   - Sends a conversation's context to the API, including a system prompt and user prompt by using the `chat.completions.create()` method.\n",
       "\n",
       "4. **Streaming Responses**:\n",
       "   - The responses are streamed in chunks, allowing incremental processing and display.\n",
       "\n",
       "5. **Display Updates**:\n",
       "   - Accumulates the content of the response, cleans it up by removing code blocks and \"\" indicators, and updates the notebook display in real-time.\n",
       "\n",
       "This function is intended for interactive applications where immediate feedback from the Llama model is needed.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_answer_from_open_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8148ca18-9e1d-49df-b169-3b3198c7f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, update_display\n",
    "\n",
    "def get_answer_from_llama():\n",
    "    ollama_via_openai = OpenAI(\n",
    "        base_url='http://localhost:11434/v1',  # Ollama's API endpoint\n",
    "        api_key='ollama'  # placeholder API key for local Ollama\n",
    "    )\n",
    "\n",
    "    stream = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    accumulated = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content or \"\"\n",
    "        accumulated += delta\n",
    "        accumulated_clean = accumulated.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(accumulated_clean), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c431879c-8ea3-4480-ad6d-c56ee50040e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Analysis\n",
       "--------\n",
       "\n",
       "### Overview\n",
       "\n",
       "The provided code snippet is designed to interact with the Ollama AI model, utilizing the OpenAI API. It creates a chat session, providing system prompts (e.g., the context of the conversation), user input prompts, and processes the responses output by the Ollama model.\n",
       "\n",
       "### Key Functionality\n",
       "\n",
       "1. **Ollama API Setup**: The code initializes an instance of `OpenAI` with the base URL for Ollama's API endpoint and a placeholder API key.\n",
       "2. **Chat Session Establishment**: It creates a chat session using the established API connection, specifying the Ollama model to be used (`MODEL_LLAMA`), as well as system and user prompts within the session.\n",
       "3. **Response Processing**: The response from the Ollama model is received in chunks (due to `stream=True`). Each chunk is processed by appending its content to an accumulated string, which represents the completed chat response.\n",
       "\n",
       "### Code Quality and Best Practices\n",
       "\n",
       "*   **Modularity**: The function is straightforward and effectively isolated into individual components for managing the chat session with the Ollama model.\n",
       "*   **API Documentation**: Usage of OpenAI API follows general guidelines by including relevant parameters within the `completions.create` call. This suggests adherence to available documentation.\n",
       "\n",
       "### Security Considerations\n",
       "\n",
       "1.  **API Key Handling**: By using a placeholder, secure key should be used in production environments rather than hardcoded values.\n",
       "2.  **Session Management**: If errors or failures occur during the chat completion process, handle these exceptions appropriately and maintain application stability.\n",
       "3.  **Handling User Input**: For Ollama usage with potential user interactions (like text from user), thoroughly validate provided data to prevent abuse (if that's a possibility).\n",
       "\n",
       "### Further Recommendations\n",
       "\n",
       "*   The use of `stream=True` indicates that the entire response might not be consumed at once because it is an asynchronous process. Thus, ensuring reliable handling of errors or chunk size issues would be essential.\n",
       "*   As is with any Ollama usage in production, secure keys, including API secret keys in place and potentially encrypt sensitive information for safety.\n",
       "\n",
       "### Possible Improvement\n",
       "\n",
       "To improve this function, more data can be used for testing like a specific user prompt. It may help make the program more interactive when responding from user input and improve understanding of how accurately the chat works towards real-world application use cases with AI based systems"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_answer_from_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17ba8714-06b5-4399-9e2a-65cfaa2b3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def get_answer_from_llama():\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.2\",  # your model name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a251e187-2364-43ce-b39c-cef7829dbe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Code Analysis**\n",
      "\n",
      "The provided code snippet is written in Python and utilizes the `yield from` statement, along with a generator expression.\n",
      "\n",
      "### Code Breakdown\n",
      "```python\n",
      "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "```\n",
      "\n",
      "Here's what each part does:\n",
      "\n",
      "* `{...}`: This is an immediately invoked dictionary comprehension, creating a new dictionary (`result_dict`) that contains the results of the generator expression.\n",
      "* `for book in books`: This iterates over the `books` iterable (presumably a list or other collection).\n",
      "* `if book.get(\"author\")`: This filters out any `book` objects that do not have an `\"author\"` key. The `.get()` method returns the value associated with the specified key, defaulting to `None` if it doesn't exist.\n",
      "* `{...}`: Again, a dictionary comprehension that iterates over the filtered list of books.\n",
      "\n",
      "### What the Code Does\n",
      "The code generates a new list (`result_dict`) containing only the authors from the `books` collection. It uses a generator expression within another dictionary comprehension to filter and extract the required data.\n",
      "\n",
      "However, the use of a dictionary comprehension here is unnecessary, as it introduces an extra step without adding any benefits. A simpler approach would be:\n",
      "```python\n",
      "yield from (book.get(\"author\") for book in books if book.get(\"author\"))\n",
      "```\n",
      "\n",
      "This produces the same result but avoids creating an intermediate dictionary.\n",
      "\n",
      "### Why Use `yield from`?\n",
      "The `yield from` statement is used to simplify complex iteration logic. In this case, it allows us to delegate the iteration and filtering of `book.get(\"author\")` to a sub-generator expression, making the code more concise and readable.\n",
      "\n",
      "Note that `yield from` was introduced in Python 3.3 as a way to simplify iteration over complex iterator objects. It's particularly useful when working with libraries like `itertools` or recursive iterators.\n"
     ]
    }
   ],
   "source": [
    "get_answer_from_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a61ad-c59a-4857-8d42-f2742aa13a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
